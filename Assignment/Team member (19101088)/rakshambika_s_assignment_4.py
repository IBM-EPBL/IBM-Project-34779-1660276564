# -*- coding: utf-8 -*-
"""Rakshambika_S_Assignment_4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11QysXkjozK2NvabPwlaXjJkIM_DBNfWm

# 1 . Importing Required Package
"""

import pandas as pd 
import numpy as np 
import seaborn as sbn 
import matplotlib.pyplot as plt

"""# 2 . Loading the Dataset

"""

db = pd.read_csv('/content/Mall_Customers.csv') 
db

"""# 3. Visualizations 
## 3.1 UniVariate Analysis 
"""

plt.hist(db['Annual Income (k$)'])

plt.hist(db['Age'])

sbn.countplot(db['Age'])

sbn.countplot(db['Gender'])

plt.hist(db['Spending Score (1-100)'])

"""# 3.2 Bi-Variate Analysis"""

plt.scatter(db['Spending Score (1-100)'],db['Annual Income (k$)'])

plt.scatter(db['Gender'],db['Annual Income (k$)'])

plt.scatter(db['Age'],db['Spending Score (1-100)'])

plt.scatter(db['Age'],db['Annual Income (k$)'])

sbn.heatmap(db.corr(), annot = True)

sbn.barplot(db['Gender'], db['Age'])

"""# 3.3 Multi-Variate Analysis"""

sbn.lmplot("Spending Score (1-100)","Annual Income (k$)", db, hue="Gender", fit_reg=False);

sbn.pairplot(db)

"""# 4. Perform descriptive statistics on the dataset"""

db.describe()

db.dtypes

db.var()

db.skew()

db.corr()

db.std()

"""# 5. Check for Missing values and deal with them"""

db.isna().sum()

db.isna().sum().sum()

db.duplicated().sum()

"""# 6 . Find the outliers and replace them outliers"""

ig,ax=plt.subplots(figsize=(25,5)) 
plt.subplot(1, 5, 2) 
sbn.boxplot(x=db['Age']) 
plt.subplot(1, 5, 3) 
sbn.boxplot(x=db['Annual Income (k$)']) 
plt.subplot(1, 5, 4) 
sbn.boxplot(x=db['Spending Score (1-100)']) 
plt.subplot(1, 5, 1) 
sbn.boxplot(x=db['CustomerID'])

q=db.quantile(q = [0.25, 0.75]) 
q

q.loc[0.75]

q.loc[0.25]

IQR=q.iloc[1]-q.iloc[0] 
IQR

upper=q.iloc[1] + (1.5 *IQR) 
upper

lower=q.iloc[0] - (1.5* IQR) 
lower

db.mean()

db['Annual Income (k$)'].max()

sbn.boxplot(db['CustomerID'])

sbn.boxplot(db['Age'])

sbn.boxplot(db['Annual Income (k$)'])

sbn.boxplot(db['Spending Score (1-100)'])

"""#7. Check for Categorical columns and perform encoding"""

db.select_dtypes(include='object').columns

db['Gender'].unique()

db['Gender'].replace({'Male':1,'Female':0},inplace=True) 
db

db.head()

"""# 8 . Scaling the data"""

from sklearn.preprocessing import StandardScaler 
ss = StandardScaler().fit_transform(db)
ss

"""# 9 . Perform any of the clustering algorithms"""

from sklearn.cluster import KMeans 
TWSS = [] 
k = list(range(2,9)) 
for i in k: 
  kmeans=KMeans(n_clusters=i, init='k-means++') 
  kmeans.fit(db) 
  TWSS.append(kmeans.inertia_) 
  TWSS

plt.plot(k,TWSS, 'ro--') 
plt.xlabel('No of Clusters') 
plt.ylabel('TWSS')

model = KMeans(n_clusters = 4) 
model.fit(db)

mb = pd.Series(model.labels_) 
db['Cluster'] = mb 
db

mb=pd.Series(model.labels_) 
db.head(3)

"""# 10 . Add the cluster data with the primary dataset"""

db['Cluster']=kmeans.labels_ 
db.head()

db.tail()

"""# 11 . Split the data into dependent and independent variables"""

X=db.drop('Cluster',axis=1) 
Y=db['Cluster'] 
y=db['Cluster'] 
y

from sklearn.model_selection import train_test_split 
X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state =42)
print("Number transactions X_train dataset: ", X_train.shape) 
print("Number transactions y_train dataset: ", y_train.shape) 
print("Number transactions X_test dataset: ", X_test.shape) 
print("Number transactions y_test dataset: ", y_test.shape)

"""# 12 . Split the data into training and testing"""

X_train

X_test

y_train

y_test

"""# 13 . Build the Model"""

from sklearn.linear_model import LogisticRegression 
model=LogisticRegression() 
model.fit(X_train, y_train)

"""# 14. Train the Model"""

model.score(X_train,y_train)

"""# 15 . Test the Model"""

model.score(X_test,y_test)

"""# 16. Measure the performance using Evaluation Metrics"""

from sklearn.metrics import confusion_matrix,classification_report 
y_pred=model.predict(X_test) 
confusion_matrix(y_test,y_pred)